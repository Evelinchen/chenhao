XGBoost如何进行特征选择和特征提取<br/>
===


Gradient Boosted Decision Tree<br>
===
属性：<br/>
  boosting算法的工作机制是先从训练集用 初始权重训练出一个弱学习器1,根据若学习器的学习误差表现更新训练样本的权重，使之前在弱学习学习器上误差高的样本点的权重变高，在新权重的样本集上继续训练学习器2，如此循环，直到学习器数目达到指定书目T。<br/>
step:<br/>
  1.每轮bootstrap后都要更新样本D中，每个样本的权重<br/>
  2.在更新权的样本上继续训练g_t树(Tip:每一轮训练出的g_t都是一个弱决策器)<br/>
  3.每轮需要计算出g_t的权重：

集成学习
===


